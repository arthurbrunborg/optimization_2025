{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5cdb0c7",
   "metadata": {},
   "source": [
    "# Second Wolfe conditions\n",
    "\n",
    "## Introduction to optimization and operations research.\n",
    "\n",
    "Michel Bierlaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e433a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import fsolve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd722600",
   "metadata": {},
   "source": [
    "In this lab, you will practice the **second Wolfe condition**\n",
    "in an inexact line search. Starting from a quadratic objective and an initial point, you\n",
    "will (i) compute the gradient and Hessian, (ii) form Newton’s direction and verify it is a\n",
    "**descent direction** via the directional derivative, and (iii) study how the ratio\n",
    "⟨∇f(x_α), d_N⟩ / ⟨∇f(x_0), d_N⟩ behaves along the line. You will plot this ratio, locate the\n",
    "α values that satisfy the condition for a given β₂, and compare them to the minimizer along\n",
    "the line. The goal is to connect the algebra (∇f, ∇²f), the geometry of line search, and the\n",
    "practical choice of step sizes that guarantee sufficient curvature decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ebd62",
   "metadata": {},
   "source": [
    "Consider the unconstrained optimization problem\n",
    "$$\\min_{x \\in \\mathbb{R}^2} f(x)=x_1^2+x_1x_2+2x_2^2,\n",
    "$$\n",
    "and the point $x_0=(1,1)^T$.\n",
    "\n",
    "\n",
    "- Calculate Newton's direction at $x_0$.\n",
    "- Verify that it is a descent direction.\n",
    "- Consider the second Wolfe condition with $\\beta_2=0.7$. What are\n",
    "the values of the step $\\alpha$ that verify the condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174f1cc",
   "metadata": {},
   "source": [
    "First, implement the function and its derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def f(x: np.array) -> float:\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    # Python starts the numbering at zero\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    result = x_1 * x_1 + x_1 * x_2 + 2 * x_2 * x_2\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_zero = np.array([1.0, 1.0])\n",
    "f_zero = f(x_zero)\n",
    "print(f'f({x_zero}) = {f_zero}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce54ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x: np.array) -> np.array:\n",
    "    \"\"\"Gradient of the objective function\"\"\"\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    g_1 = 2 * x_1 + x_2\n",
    "    g_2 = x_1 + 4 * x_2\n",
    "    return np.array([g_1, g_2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f29e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_zero = gradient(x_zero)\n",
    "print(f'Gradient of f({x_zero}) = {g_zero}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f04c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x: np.array) -> np.array:\n",
    "    \"\"\"Second derivative matrix of the objective function\"\"\"\n",
    "    # In this case, the hessian does not depend on x\n",
    "    x_1 = x[0]\n",
    "    x_2 = x[1]\n",
    "    h_1_1 = 2\n",
    "    h_1_2 = 1\n",
    "    h_2_1 = 1\n",
    "    h_2_2 = 4\n",
    "    h = np.array([[h_1_1, h_1_2], [h_2_1, h_2_2]])\n",
    "    return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2145b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zero = hessian(x_zero)\n",
    "print(f'Hessian of f({x_zero}) =\\n{h_zero}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57bd38",
   "metadata": {},
   "source": [
    "Note that there exists Python packages for automatic differentiation,\n",
    "such as ``autograd``or ``jax``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb595bbf",
   "metadata": {},
   "source": [
    "Calculate Newton's direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_direction = np.linalg.solve(h_zero, -g_zero)\n",
    "print(f\"Newton's direction: {newton_direction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7660c01",
   "metadata": {},
   "source": [
    "Verify that it is a descent direction.\n",
    "We calculate the directional derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b97e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "directional_derivative = np.inner(newton_direction, g_zero)\n",
    "print(f'Directional derivative: {directional_derivative}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6bc1f2",
   "metadata": {},
   "source": [
    "It must be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if directional_derivative < 0:\n",
    "    print('Descent direction')\n",
    "else:\n",
    "    print('Not a descent direction')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f6e6d",
   "metadata": {},
   "source": [
    "Write the function that associates a step alpha along Newton's direction with the value of\n",
    "the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(alpha: float) -> float:\n",
    "    \"\"\"\n",
    "\n",
    "    :param alpha: step along the direction\n",
    "    :return: value of the objective function\n",
    "    \"\"\"\n",
    "    new_point = x_zero + alpha * newton_direction\n",
    "    return f(new_point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c71f83",
   "metadata": {},
   "source": [
    "We plot the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41067bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = np.linspace(0, 2.5, 100)\n",
    "objective_values = [linesearch(alpha) for alpha in alpha_values]\n",
    "plt.plot(alpha_values, objective_values)\n",
    "plt.xlabel('Step alpha')\n",
    "plt.ylabel('Objective Function Value')\n",
    "plt.title('Line Search Plot')\n",
    "plt.grid(True)\n",
    "plt.ylim(top=4)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8358c39",
   "metadata": {},
   "source": [
    "Consider the second Wolfe condition with $\\beta_2=0.7$.\n",
    "$$\n",
    "\\nabla f(x_\\alpha)^Td_N \\geq \\beta_2 \\nabla f(x_0)^Td_N.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18588e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_2 = 0.7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9174c",
   "metadata": {},
   "source": [
    "Consider this equivalent version of the same condition.\n",
    "$$\n",
    "\\frac{\\nabla f(x_\\alpha)^Td_N}{\\nabla f(x_0)^Td_N} \\leq \\beta_2.\n",
    "$$\n",
    "Note that the inequality has changed because $\\nabla f(x_0)^Td_N < 0$.\n",
    "\n",
    "We defined the function calculating that ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_wolfe(alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    Second wolfe condition\n",
    "\n",
    "    :param alpha: step along the direction\n",
    "    :return: ratio of the second Wolfe condition\n",
    "    \"\"\"\n",
    "    new_point = x_zero + alpha * newton_direction\n",
    "    numerator = np.inner(\n",
    "        gradient(new_point), newton_direction\n",
    "    )\n",
    "    denominator = directional_derivative\n",
    "    return numerator / denominator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91563461",
   "metadata": {},
   "source": [
    "Plot the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b06c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wolfe_values = [second_wolfe(alpha) for alpha in alpha_values]\n",
    "plt.plot(alpha_values, objective_values)\n",
    "plt.plot(alpha_values, wolfe_values)\n",
    "plt.axhline(y=beta_2, color='blue', linestyle='--')\n",
    "plt.text(\n",
    "    alpha_values[-1], beta_2, f'beta_2={beta_2}', color='blue', ha='right', va='bottom'\n",
    ")\n",
    "plt.xlabel('Step alpha')\n",
    "plt.ylabel('Objective Function Value')\n",
    "plt.title('Line Search Plot')\n",
    "plt.grid(True)\n",
    "plt.ylim(top=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de9a2a",
   "metadata": {},
   "source": [
    "The line should intersect two points:\n",
    "\n",
    "- At $\\alpha=0$, it is equal to 1.\n",
    "- At $\\alpha$ corresponding to the minimum of the function, it is equal to 0. Indeed, the directional\n",
    "derivative is zero for this value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7344e86",
   "metadata": {},
   "source": [
    "What are the values of the step $\\alpha$ that verify the condition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80902995",
   "metadata": {},
   "source": [
    "We need to find the values of alpha such that the difference is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(alpha: float) -> float:\n",
    "    \"\"\"\n",
    "    Difference between the first wolfe condition and the function\n",
    "\n",
    "    :param alpha: step along the direction\n",
    "    :return: Wolfe condition\n",
    "    \"\"\"\n",
    "    return beta_2 - second_wolfe(alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5ebf9",
   "metadata": {},
   "source": [
    "Find the root of that function. Use the function `fsolve` from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = 0.25\n",
    "root = fsolve(difference, guess)[0]\n",
    "print(f'Point where the ratio equals beta_2: {root:.2g}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8069bde",
   "metadata": {},
   "source": [
    "Plot the function with the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3edc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha_values, objective_values)\n",
    "plt.plot(alpha_values, wolfe_values)\n",
    "plt.axvline(root, color='red', linestyle='--')\n",
    "plt.axhline(y=beta_2, color='blue', linestyle='--')\n",
    "plt.text(\n",
    "    alpha_values[-1], beta_2, f'beta_2={beta_2}', color='blue', ha='right', va='bottom'\n",
    ")\n",
    "plt.text(\n",
    "    root, 2.5, f'alpha={root:.2g}', color='blue', ha='left', va='bottom', rotation=90\n",
    ")\n",
    "plt.xlabel('Step alpha')\n",
    "plt.ylabel('Objective Function Value')\n",
    "plt.title('Line Search Plot')\n",
    "plt.ylim(top=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70788af4",
   "metadata": {},
   "source": [
    "The values of the step $\\alpha$ that verify the condition are\n",
    "$$ \\alpha \\geq 0.3$$."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
