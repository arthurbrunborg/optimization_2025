{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045896b6",
   "metadata": {},
   "source": [
    "# Non linear optimization: preconditioning\n",
    "\n",
    "## Introduction to optimization and operations research\n",
    "\n",
    "Michel Bierlaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aa443d",
   "metadata": {},
   "source": [
    "In this lab, you will practice **preconditioning**: a change of variables that makes the\n",
    "landscape of an objective easier for descent methods. You will compute gradients and the\n",
    "Hessian, build a **Cholesky-based** preconditioner L so that, in the new coordinates, the\n",
    "quadratic model is nearly spherical (ideally the identity), and verify how this affects the\n",
    "step computed by steepest descent (Cauchy point). The goal is to connect algebra\n",
    "(∇f, ∇²f, linear solves) with geometry (contours that become rounder), understand why\n",
    "good conditioning leads to larger, safer steps and faster convergence, and see how to\n",
    "implement the transformed gradient and Hessian correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f89205",
   "metadata": {},
   "source": [
    "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined as\n",
    "$$\n",
    "f(x)= \\frac{1}{2}x_1^2 + \\frac{101}{2} x_2^2 +  x_1 x_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9b991",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Implement a function in Python that calculates the function and its first and second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820495c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_function(x: np.array) -> tuple[float, np.array, np.array]:\n",
    "    \"\"\"Calculates the function and its derivatives\n",
    "\n",
    "    :param x: a vector of dimension 2\n",
    "    :return: a tuple with the value of the function, the gradient and the second derivatives matrix\n",
    "    \"\"\"\n",
    "    f = ...\n",
    "    g = ...\n",
    "    h = ...\n",
    "    return f, g, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d52bd",
   "metadata": {},
   "source": [
    "Test it at the point $(1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5eb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "function, gradient, hessian = the_function(x)\n",
    "print(f'f(x)={function}')\n",
    "print(f'gradient(x)={gradient}')\n",
    "print(f'hessian(x)=\\n{hessian}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef7456",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Consider a change of variables\n",
    "$$\n",
    "x' = L_k^T x,\n",
    "$$\n",
    "Consider the function in the new variables\n",
    "$$\n",
    "\\tilde{f}(x') = f(L_k^{-T} x').\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fe690",
   "metadata": {},
   "source": [
    "The gradient of the new function is\n",
    "$$ \\nabla \\tilde{f}(x') = L_k^{-1} \\nabla f(L_k^{-T} x'),$$ that is, the solution of the system:\n",
    "$$L_k \\nabla \\tilde{f}(x') = \\nabla f(L_k^{-T} x').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc767b0d",
   "metadata": {},
   "source": [
    "The hessian of the new function is\n",
    "$$ \\nabla^2 \\tilde{f}(x') = L_k^{-1} \\nabla^2 f(L_k^{-T} x') L_k^{-T}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343213b6",
   "metadata": {},
   "source": [
    "that is, the solution of the system:\n",
    "$$L_k \\nabla^2 \\tilde{f}(x') =  D^T_k,$$\n",
    "where $D_k$ is the solution of the system\n",
    "$$\\nabla^2 f(L_k^{-T} x') = L_k D_k$$\n",
    "Implement a Python function that calculates this function and its first and second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preconditioned_function(\n",
    "    x: np.array, l_k: np.array\n",
    ") -> tuple[float, np.array, np.array]:\n",
    "    \"\"\"Calculates the preconditioned function and its gradient.\n",
    "\n",
    "    :param x: a vector of dimension 2.\n",
    "    :param l_k:  matrix defining the change of variables.\n",
    "    :return: a tuple with the value of the function, the gradient and the hessian.\n",
    "    \"\"\"\n",
    "    x_original = ...\n",
    "    f, g, h = ...\n",
    "    the_gradient = ...\n",
    "    d_k = ...\n",
    "    the_hessian = ...\n",
    "    return f, the_gradient, the_hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d1102",
   "metadata": {},
   "source": [
    "Consider $L_k$ to be the Cholesky factor of the second derivative matrix\n",
    "$$\n",
    "L_k L_k^T=  \\nabla^2 f(x_k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_k = ...\n",
    "print(l_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd146e8",
   "metadata": {},
   "source": [
    "Check that it is indeed the Cholesky factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_k @ l_k.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38d4ce",
   "metadata": {},
   "source": [
    "It must be the same as the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df607960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hessian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95bd77",
   "metadata": {},
   "source": [
    "Evaluate the preconditioned function at the point $x'=L_k^T x$, where $x=(1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_x = ...\n",
    "print(f'{prec_x=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eee386",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_function, prec_gradient, prec_hessian = preconditioned_function(prec_x, l_k)\n",
    "print(f'Preconditioned f(x)={prec_function}')\n",
    "print(f'Preconditioned gradient(x)={prec_gradient}')\n",
    "print(f'Preconditioned hessian(x)=\\n{prec_hessian}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f5a1c",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Apply one iteration of the steepest descent\n",
    "algorithm on $\\tilde{f}$ from that point, that is\n",
    "$$\n",
    "x'_{k+1} = x'_k - \\alpha \\nabla \\tilde{f}(x'_k),\n",
    "$$\n",
    "where the step size is\n",
    "$$\n",
    "\\alpha = \\frac{\\nabla \\tilde{f}(x'_k)^T \\nabla \\tilde{f}(x'_k)}{\\nabla\n",
    "\\tilde{f}(x'_k)^T \\nabla^2 \\tilde{f}(x'_k) \\nabla \\tilde{f}(x'_k)}.\n",
    "$$\n",
    "It is the Cauchy point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ...\n",
    "\n",
    "\n",
    "print(f'{alpha=:.3g}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12357ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prec_x = ...\n",
    "print(f'{new_prec_x=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8bc24",
   "metadata": {},
   "source": [
    "Identify the corresponding point in the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd0c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = ...\n",
    "print(f'{new_x=}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
